import { openai } from '@ai-sdk/openai';
import { convertToModelMessages, streamText, UIMessage } from 'ai';
// Import OpenAI SDK helper and utility functions:
// - openai: used to select a specific OpenAI model
// - convertToModelMessages: converts UIMessage array to the model's required format
// - streamText: allows streaming AI responses in real-time
// - UIMessage: type representing a message from the UI

export async function POST(req: Request) {
  // Define an async POST handler for the API route
  try {
    const { messages }: { messages: UIMessage[] } = await req.json();
    // Parse the incoming request JSON
    // Expecting an object with a 'messages' array of type UIMessage[]

    const result = streamText({
      model: openai('gpt-4.1-nano'),
      // Select the GPT-4.1 Nano model

      messages: [
        {
          role: 'system', // System-level instruction to shape model behavior
          content:
            // This prompt defines how the AI should respond throughout the conversation
            // 'You are a helpful coding assistant. Keep responses under 3 sentences and focus on practical examples.',
            'You are a friendly teacher who explains concepts using simple analogies. Always relate technical concepts to everyday experiences.',
        },
        ...convertToModelMessages(messages),
        // Spread the user's existing UI messages after the system prompt
        // Ensures the model reads the system instruction first, then user/assistant messages
      ],
    });

    result.usage.then((usage) => {
      // After the streaming completes, we can inspect token usage
      // usage contains inputTokens, outputTokens, and totalTokens
      // Example structure: { messageCount: 3, inputTokens: 68, outputTokens: 7, totalTokens: 75 }
      console.log({
        messageCount: messages.length, // Number of user messages sent
        inputTokens: usage.inputTokens, // Tokens consumed by input
        outputTokens: usage.outputTokens, // Tokens generated by the AI
        totalTokens: usage.totalTokens, // Total usage (input + output)
      });
    });

    return result.toUIMessageStreamResponse();
    // Return a Response that streams the AI output to the frontend in real-time
  } catch (error) {
    console.error('Error streaming chat completion:', error);
    // Log any error that occurs during processing

    return new Response('Failed to stream chat completion', { status: 500 });
    // Return a 500 error to the client if something went wrong
  }
}
